{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c11bc08-a667-43ba-9331-a9c34d8c20b0",
   "metadata": {},
   "source": [
    "# Language Model Similarity Example\n",
    "\n",
    "As discussed in the anchors notebook, a similarity anchor allows you to provide arbitrary function code to calculate similarity between user specified text and the passed data.\n",
    "\n",
    "This notebook shows how to provide a language model to a similarity anchor, allowing the utilization of knowledge inside embedding spaces as part of the ICAT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67719045-d771-4e17-bb1a-5f1bbed741be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change these constants as needed based on your hardware constraints\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = \"cuda\"\n",
    "MODEL_NAME = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb11a01-cf2f-4f67-9803-007a280bd4e2",
   "metadata": {},
   "source": [
    "For simplicity, we load in (by default, based on constant above) a basic BERT pre-trained model and do no further fine-tuning. In principle of course, any transformer can be supplied here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddeed16-a897-4fed-ab8e-ae89a013f68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "text_model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47941b7-b7f7-4d8d-843f-676542f5b443",
   "metadata": {},
   "source": [
    "To use the language model, we define a function that takes a dataframe (containing all the texts we want to analyze) and the anchor instance (through which we can find the target text we're finding similarity with respect to.) \n",
    "\n",
    "An anchorlist instance has a `cache` dictionary that we'll store all of the transformer embeddings in _once_, so that all `featurize()` calls after the first one will be much faster. This `cache` dictionary is also pickled and unpickled when the anchorlist is saved and loaded, so it can persist across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c142a2-def6-4cb8-8e1f-cebc8010adcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import icat\n",
    "\n",
    "def lm_similarity(data: pd.DataFrame, anchor: icat.SimilarityFunctionAnchor):\n",
    "    anchor_list = anchor.container  # we get the containing anchor list so we can\n",
    "                                    # store and retrieve values from its `cache`\n",
    "    target_text = anchor.reference_texts[0]  # the target text we're computing similarity to.\n",
    "                                             # Note that for simplicity we only use the first\n",
    "                                             # referenced text, but in principle this function\n",
    "                                             # could be implemented to handle multiple targets,\n",
    "                                             # e.g. use the average embedding.\n",
    "    source_text = data[anchor.text_col].tolist()\n",
    "    \n",
    "    # if we haven't computed the embeddings for the dataframe yet, do so now.\n",
    "    # NOTE: this works on the assumption that the dataset isn't going to change,\n",
    "    # special considerations are required if we put new data through this function\n",
    "    if f\"similarity_embeddings_{MODEL_NAME}\" not in anchor_list.cache:\n",
    "        embedded_batches = []\n",
    "        \n",
    "        # run the tokenizer and model embedding on batches\n",
    "        max_batches = data.shape[0] // BATCH_SIZE + 1\n",
    "        last_batch = data.shape[0] // BATCH_SIZE\n",
    "        for batch in range(max_batches):\n",
    "            # compute range for this batch\n",
    "            batch_start = batch * BATCH_SIZE\n",
    "            batch_end = data.shape[0] if batch == last_batch else batch_start + BATCH_SIZE\n",
    "            \n",
    "            # get the texts within the batch range\n",
    "            batch_text = source_text[batch_start:batch_end]\n",
    "            \n",
    "            # tokenize and embed with the model\n",
    "            tokenized = tokenizer(\n",
    "                batch_text, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                padding=\"max_length\",\n",
    "            )[\"input_ids\"].to(DEVICE).detach()\n",
    "            text_embeddings = text_model(tokenized).last_hidden_state.detach().cpu().numpy()\n",
    "            embedded_batches.append(text_embeddings)\n",
    "            \n",
    "        # stack all the embeddings and average the token embeddings to get the full text \n",
    "        # representation for each\n",
    "        embeddings = np.concatenate(embedded_batches, axis=0)\n",
    "        embeddings = embeddings.mean(axis=1)\n",
    "        \n",
    "        anchor_list.cache[f\"similarity_embeddings_{MODEL_NAME}\"] = embeddings\n",
    "    \n",
    "    # tokenize and get the full text embedding for the target text\n",
    "    tokenized_target = tokenizer(\n",
    "        batch_text, \n",
    "        return_tensors='pt', \n",
    "        truncation=True, \n",
    "        padding=\"max_length\",\n",
    "    )[\"input_ids\"].to(DEVICE).detach()\n",
    "    target_embedding = text_model(tokenized_target).last_hidden_state.detach().cpu().numpy()\n",
    "    target_embedding = target_embedding.mean(axis=1)\n",
    "    \n",
    "    # compute cosine similarity between the target text embedding and all the embeddings\n",
    "    # from the dataframe\n",
    "    similarities = cosine_similarity(target_embedding, anchor_list.cache[f\"similarity_embeddings_{MODEL_NAME}\"])\n",
    "    \n",
    "    # massage the similarity values a little to get better spread in the visualization \n",
    "    # and put a minimum threshold on \"activation\"\n",
    "    similarities = similarities * 2 - 1\n",
    "    similarities[similarities < 0.25] = 0.0\n",
    "    \n",
    "    return pd.Series(similarities[0], index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa9719-c708-408f-84cf-421df9c8e46c",
   "metadata": {},
   "source": [
    "We load in a dataset to work with and initialize panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35c926-97f2-49c2-a80e-01ad42d907a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "pn.extension('vega')\n",
    "\n",
    "dataset = fetch_20newsgroups(subset=\"train\")\n",
    "df = pd.DataFrame({\"text\": dataset[\"data\"], \"category\": [dataset[\"target_names\"][i] for i in dataset[\"target\"]]})\n",
    "#df = df.iloc[0:2000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5d51f-ad83-496e-9eeb-9edbcec98ca2",
   "metadata": {},
   "source": [
    "Now we create a model and pass our similarity function into the constructor. Internally, `model.similarity_functions` resolves to a dictionary, where the key is the name of the function. These keys are what display in the dropdown of the `SimilarityFunctionAnchor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8eec49-a4e5-42b9-85f7-174e236e0968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = icat.Model(df, text_col=\"text\", similarity_functions=[lm_similarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3880dc-554e-48f9-bd16-67b3487f19b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6bfd1-5632-4038-85f6-89e76231eb27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
