{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c11bc08-a667-43ba-9331-a9c34d8c20b0",
   "metadata": {},
   "source": [
    "# Language Model Similarity Example\n",
    "\n",
    "As discussed in the anchors notebook, a similarity anchor allows you to provide arbitrary function code to calculate similarity between user specified text and the passed data.\n",
    "\n",
    "This notebook shows how to provide a language model to a similarity anchor, allowing the utilization of knowledge inside embedding spaces as part of the ICAT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67719045-d771-4e17-bb1a-5f1bbed741be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change these constants as needed based on your hardware constraints\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = \"cuda\"\n",
    "MODEL_NAME = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb11a01-cf2f-4f67-9803-007a280bd4e2",
   "metadata": {},
   "source": [
    "For simplicity, we load in (by default, based on constant above) a basic BERT pre-trained model and do no further fine-tuning. In principle of course, any transformer can be supplied here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddeed16-a897-4fed-ab8e-ae89a013f68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "text_model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47941b7-b7f7-4d8d-843f-676542f5b443",
   "metadata": {},
   "source": [
    "To use the language model, we define a function that takes a dataframe (containing all the texts we want to analyze) and the anchor instance (through which we can find the target text we're finding similarity with respect to.) \n",
    "\n",
    "An anchorlist instance has a `cache` dictionary that we'll store all of the transformer embeddings in _once_, so that all `featurize()` calls after the first one will be much faster. This `cache` dictionary is also pickled and unpickled when the anchorlist is saved and loaded, so it can persist across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c142a2-def6-4cb8-8e1f-cebc8010adcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import icat\n",
    "\n",
    "class LMSimAnchor(icat.anchors.SimilarityAnchorBase):\n",
    "    NAME = \"BERT\"  # optional attribute to set default name inside ICAT UI\n",
    "    DESCRIPTION = \"Use cosine similarity between BERT embeddings of data and target.\"\n",
    "    # optional attribute to set description of anchor type in ICAT UI\n",
    "    \n",
    "    def embed(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"This function takes some set of data and embeds the text column using\n",
    "        the transformer model stored in ``text_model``.\"\"\"\n",
    "        embedded_batches = []\n",
    "        \n",
    "        # run the tokenizer and model embedding on batches\n",
    "        max_batches = data.shape[0] // BATCH_SIZE + 1\n",
    "        last_batch = data.shape[0] // BATCH_SIZE\n",
    "        for batch in range(max_batches):\n",
    "            # compute range for this batch\n",
    "            batch_start = batch * BATCH_SIZE\n",
    "            batch_end = data.shape[0] if batch == last_batch else batch_start + BATCH_SIZE\n",
    "\n",
    "            # get the texts within the batch range\n",
    "            batch_text = data[self.text_col].tolist()[batch_start:batch_end]\n",
    "\n",
    "            # tokenize and embed with the model\n",
    "            tokenized = tokenizer(\n",
    "                batch_text, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                padding=\"max_length\",\n",
    "            )[\"input_ids\"].to(DEVICE).detach()\n",
    "            text_embeddings = text_model(tokenized).last_hidden_state.detach().cpu().numpy()\n",
    "            embedded_batches.append(text_embeddings)\n",
    "            \n",
    "        # stack all the embeddings and average the token embeddings to get the full text \n",
    "        # representation for each\n",
    "        embeddings = np.concatenate(embedded_batches, axis=0)\n",
    "        embeddings = embeddings.mean(axis=1)\n",
    "        embeddings_df = pd.DataFrame(embeddings, index=data.index)\n",
    "        return embeddings_df\n",
    "\n",
    "    def featurize(self, data: pd.DataFrame) -> pd.Series:\n",
    "        target_text = self.reference_texts[0] # the target text we're computing similarity to.\n",
    "                                              # Note that for simplicity we only use the first\n",
    "                                              # referenced text, but in principle this function\n",
    "                                              # could be implemented to handle multiple targets,\n",
    "                                              # e.g. use the average embedding.\n",
    "        \n",
    "        # determine data that hasn't been embedded yet, note that we determine this exclusively \n",
    "        # by index\n",
    "        to_embed = data\n",
    "        cache_key = f\"similarity_embeddings_{MODEL_NAME}\"\n",
    "        if cache_key in self.global_cache:\n",
    "            to_embed = data[~data.index.isin(self.global_cache[cache_key].index)]\n",
    "        else:\n",
    "            # make sure the series exists to place our embeddings into later\n",
    "            self.global_cache[cache_key] = pd.DataFrame()\n",
    "            \n",
    "        # perform any necessary embeddings and store into global cache.\n",
    "        if len(to_embed) > 0:\n",
    "            self.global_cache[cache_key] = pd.concat([self.global_cache[cache_key], self.embed(to_embed)])\n",
    "            \n",
    "        # tokenize and get the full text embedding for the target text\n",
    "        tokenized_target = tokenizer(\n",
    "            target_text, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            padding=\"max_length\",\n",
    "        )[\"input_ids\"].to(DEVICE).detach()\n",
    "        target_embedding = text_model(tokenized_target).last_hidden_state.detach().cpu().numpy()\n",
    "        target_embedding = target_embedding.mean(axis=1)\n",
    "\n",
    "        # compute cosine similarity between the target text embedding and all the embeddings\n",
    "        # from the dataframe\n",
    "        similarities = cosine_similarity(target_embedding, self.global_cache[cache_key].loc[data.index].values)\n",
    "\n",
    "        # massage the similarity values a little to get better spread in the visualization \n",
    "        # and put a minimum threshold on \"activation\"\n",
    "        similarities = similarities * 2 - 1\n",
    "        similarities[similarities < 0.25] = 0.0\n",
    "\n",
    "        return pd.Series(similarities[0], index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa9719-c708-408f-84cf-421df9c8e46c",
   "metadata": {},
   "source": [
    "We load in a dataset to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35c926-97f2-49c2-a80e-01ad42d907a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(subset=\"train\")\n",
    "df = pd.DataFrame({\"text\": dataset[\"data\"], \"category\": [dataset[\"target_names\"][i] for i in dataset[\"target\"]]})\n",
    "#df = df.iloc[0:1999]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a3c0d-0fef-4d6d-8f94-cf5a1f2cfb41",
   "metadata": {},
   "source": [
    "ICAT has to be initialized before use, taking care of things like panel and pre-requisite UI setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d7254-e8ae-488c-bf65-38c18fe55cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "icat.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5d51f-ad83-496e-9eeb-9edbcec98ca2",
   "metadata": {},
   "source": [
    "Now we create a model to explore with. ICAT's \"anchor types\" tab will automatically detect any `Anchor` class definitions, allowing you to dynamically add that anchor type to work with directly within the interface. Alternatively, you can pass the types you want to use to the constructor, or subsequently call `model.anchor_list.add_anchor_type(LMSimAnchor)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8eec49-a4e5-42b9-85f7-174e236e0968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = icat.Model(df, text_col=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3880dc-554e-48f9-bd16-67b3487f19b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4602ea5-3bfd-4298-9f4e-b4abb5ed00c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(\"wip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23697512-5ade-488e-8c18-ebc168e4d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
